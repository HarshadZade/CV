{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a80ee45a7dde763ed37979dfddb89bea",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "## Neural Networks for Recognition - Assignment 3\n",
    "    Instructor: Kris                          TAs: Arka, Jinkun, Rawal, Rohan, Sheng-Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d23ab52a5796705fd330208f55a1aca",
     "grade": false,
     "grade_id": "cell-ee45598a54db40ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "## Theory Questions (45 points)\n",
    "**Grading**:  \n",
    "- The theory part consists of 7 questions.\n",
    "- Please add your answers to the writeup (submitted as pdf to HW3:PDF). Insert images whenever necessary.\n",
    "- Show all your work to obtain full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "331415a2eb1c44f27d792e9bfe06b47a",
     "grade": false,
     "grade_id": "theory_q2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 (3 points)\n",
    "\n",
    "The softmax function can be defined as, $softmax(x_i)= \\frac{1}{S} s_i$ where $s_i = e^{x_i}$ , $S=\\sum_i s_i$. Using this definition, please answer Q1.1, Q1.2 and Q1.3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d5739e851b6472a72a8dab400606ed0",
     "grade": false,
     "grade_id": "cell-82ce863ef815b3b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, what are the properties of $softmax(x)$, specifically, what is the range of each element in $softmax(x)$? What is the sum of all elements in $softmax(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61aacdeda627affd5fc5845996f1d72d",
     "grade": true,
     "grade_id": "theory_q2_ans",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Softmax is a function that turns a given vector into a vector of same dimensions such that the sum of the elements of the vector is 1. Each element in the input vector can vary from $[-\\infty, +\\infty]$. Each element in the output is in the range [0,1] and the sum of all the elements in output = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e601d9a474cd8238bfd928279e38c4d",
     "grade": false,
     "grade_id": "cell-11b4b5748e0010bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2 (1 point)\n",
    "”Softmax takes an arbitrary real valued vector $x$ and turns it into a ___”. **Please fill in the blank using an appropriate word/phrase**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed91437533a9e58883df69f77ea17eac",
     "grade": true,
     "grade_id": "cell-433a8c3d311f4d7a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "probability distribution of all the elements in x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "705218fbf3e7d15fbc223f44a096b08a",
     "grade": false,
     "grade_id": "cell-db431f37e7386bd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.3 (1 point)\n",
    "Let $x \\in \\mathbb{R}^d$, assume $v = softmax(softmax(... softmax(x)))$ where the softmax function is applied to $x$ recursively $N$ times. What is the value of $v$ as a function of $d$ $\\forall x \\in \\mathbb{R}^d$, in the limit $N \\rightarrow \\infty$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2aaabd020a614000076e273fff6e2bd",
     "grade": true,
     "grade_id": "cell-a52d68820bc5f158",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1/d <br/>\n",
    "Each element in x will have equal probability  = 1/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.1 (3 points, write-up)\n",
    "Why is it not a good idea to initialize a network with all zeros? If you imagine that every layer has weights and biases, what can a zero-initialized network output after training?\n",
    "\n",
    "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing all the weights in the network with zero will not give us any gradient (gradient=0) therefore we wont be able to train anything since we havent learned any information. \n",
    "The output after training will be nothing but all zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.1.3 (2 points, write-up)\n",
    "Why is it a good practice to initialize the parameters using random numbers? Explain the intuition behind scaling the initializations depending on layer size (see near Fig 6 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))?\n",
    "\n",
    "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By initializing the parameters using random numbers, we make sure that the gradient is not approaching the same local minima and getting stuck there. With different initializations, the gradient will keep tracking different minimas.  <br/>\n",
    "\n",
    "We scale the initializations so as to prevent phenomenons like vanishing gradient or explosing gradient. By having a sufficiently large variance in the initialized weights the training becomes effective, as opposed to having gradients of very different magnitudes at different layers that yield to ill-conditioning and slower training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b13f448e7c97c97f5029349d1b16ce0",
     "grade": false,
     "grade_id": "theory_q1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2 (4 points)\n",
    "Prove that softmax is invariant to translation, that is \n",
    "$$softmax(x) = softmax(x + c) \\qquad \\forall c \\in \\mathbb{R}$$\n",
    "Again, softmax is defined as, for each index $i$ in a vector $x$.\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "Often we use $c = − \\max x_i$. Why is that a good idea? (Tip: consider the range of values that numerator will have with $c = 0$ and $c = − \\max x_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd84960af9fd395dabf70fa038e25a73",
     "grade": true,
     "grade_id": "theory_q1_ans",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$softmax(x+c) = \\frac{e^{x_i+c}}{\\sum_j e^{x_j+c}}\n",
    "               = \\frac{e^{x_i}e^c}{\\sum_j e^{x_j}e^c}\n",
    "               = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "               =  softmax(x) $$\n",
    "\n",
    "Adding negative of the maximum value ensures that the exponential does not blow up. In case of a very large value the exponential will have a very high value. But the adding of -ve term, we bring down the exponential term and thus get resonable values. Had we not done this, i.e., for c = 0, the range of values that the numerator will have will vary from very small to way too high values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1\n",
    "<!-- ![alt-text-1](Q2_1accuracy.png) ![alt-text2](Q2_1loss.png) -->\n",
    "Validation accuracy:  0.725\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q2_1accuracy.png)  |  ![](Q2_1loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q2.1\n",
    "import math\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "max_iters = 50\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 3\n",
    "learning_rate = 1e-3\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# raise NotImplementedError()\n",
    "hidden_size = 64\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers (named \"layer1\" and \"output\") here\n",
    "# YOUR CODE HERE\n",
    "initialize_weights(1024,hidden_size,params,'layer1')\n",
    "initialize_weights(hidden_size,36,params,'output')\n",
    "# raise NotImplementedError()\n",
    "train_acc_arr = []\n",
    "train_loss_arr = []\n",
    "valid_acc_arr = []\n",
    "valid_loss_arr = []\n",
    "itr_arr = []\n",
    "# with default settings, you should get loss < 150 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    itr_arr.append(itr)\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    for xb,yb in batches:\n",
    "        # print(\"xb shape = \",xb.shape)\n",
    "        # training loop can be exactly the same as q2!\n",
    "        # YOUR CODE HERE\n",
    "        post_act = forward(xb,params,'layer1',sigmoid)\n",
    "        pred_output = forward(post_act,params,'output',softmax)\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals\n",
    "        # YOUR CODE HERE\n",
    "        loss, acc = compute_loss_and_acc(yb, pred_output)\n",
    "        total_loss += loss/len(batches)\n",
    "        total_acc += acc/len(batches)\n",
    "        \n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        # backward\n",
    "        # YOUR CODE HERE\n",
    "        last_layer_backprop = backwards(pred_output - yb, params, 'output', linear_deriv)\n",
    "        hidden_layer_backprop  = backwards(last_layer_backprop, params, 'layer1', sigmoid_deriv)\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "        # apply gradient\n",
    "        # YOUR CODE HERE\n",
    "        params['Woutput'] = params['Woutput'] - learning_rate*params['grad_Woutput']\n",
    "        params['boutput'] = params['boutput'] - learning_rate*params['grad_boutput']\n",
    "        params['Wlayer1'] = params['Wlayer1'] - learning_rate*params['grad_Wlayer1']\n",
    "        params['blayer1'] = params['blayer1'] - learning_rate*params['grad_blayer1']\n",
    "            \n",
    "        # raise NotImplementedError()\n",
    "    print(\"acc = \", total_acc)\n",
    "    train_acc_arr.append(total_acc)\n",
    "    train_loss_arr.append(total_loss)\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,total_loss,total_acc))\n",
    "\n",
    "# run on validation set and report accuracy! should be above 70%\n",
    "    \n",
    "    post_act = forward(valid_x,params,'layer1',sigmoid)\n",
    "    pred_output = forward(post_act,params,'output',softmax)\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # loss\n",
    "    # be sure to add loss and accuracy to epoch totals\n",
    "    # YOUR CODE HERE\n",
    "    loss, acc = compute_loss_and_acc(valid_y, pred_output)\n",
    "    valid_loss += loss/len(batches)\n",
    "    valid_acc += acc\n",
    "    valid_acc_arr.append(valid_acc)\n",
    "    valid_loss_arr.append(valid_loss)\n",
    "    # raise NotImplementedError()\n",
    "    print('Validation accuracy: ',valid_acc)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(itr_arr,train_acc_arr, label = \"Train data\")\n",
    "plt.plot(itr_arr,valid_acc_arr, label = \"Valid data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(itr_arr,train_loss_arr, label = \"Train data\")\n",
    "plt.plot(itr_arr,valid_loss_arr, label = \"Valid data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2\n",
    "Test accuracy:  0.7633333333333333 for learning rate 5e^-3 on test data. <br/>\n",
    "With increase in learning rate, the changes in the weights are big, i.e., the accuracy and loss go haywire. The model approaches the minima of the loss function in less time but takes big steps in random directions. Whereas with  a slow learning rate, the model track the minima of the loss function very slowly and steadily with small steps that seem very smooth.\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q2_2bestLR_accuracy.png)  |  ![](Q2_2bestLR_loss.png)\n",
    "![](Q2_2bestby10LR_accuracy.png)  |  ![](Q2_2bestby10LR_loss.png)\n",
    "![](Q2_2bestinto10LR_accuracy.png)  |  ![](Q2_2bestinto10LR_loss.png)\n",
    "<!-- \n",
    "![](Q2_2bestLR_accuracy.png) <br />\n",
    "![](Q2_2bestLR_loss.png) <br />\n",
    "![](Q2_2bestby10LR_accuracy.png) <br />\n",
    "![](Q2_2bestby10LR_loss.png) <br />\n",
    "![](Q2_2bestinto10LR_accuracy.png) <br />\n",
    "![](Q2_2bestinto10LR_loss.png) <br /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3\n",
    "Initialized Weights: <br />\n",
    "![](Q2_3_initialweights.png) <br />\n",
    "Trained Weights: <br />\n",
    "![](Q2_3_trainedweights.png) <br />\n",
    "\n",
    "After the first glance one can comment that the initialized weights look all random (and they should!) and the trained weights have some form of pattern (the least one can say is the they dont look random). <br/>\n",
    "Now after a deeper look at the learned weights, we can see that there seems to be a pattern that sort of keep repeating after regular intervals. There is no way that we can back track as to what the weights truly represent, but we can be sure that they have gained values that correspond to a particular type of input. \n",
    "We can also see that at the two extremeties, there isnt much change or visible pattern. We can relate this to the type of input images that we gave for training. The main content (text in out case) lies in the central part of the image and the boundary regions are plane. And we can relate this to the trained weights picture above: way=vy pattern is absent in the extreme regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4\n",
    "<!-- Weights: <br />\n",
    "![](Q2_4_weights.png) <br />\n",
    "Image: <br />\n",
    "![](Q2_4_image.png) <br /> -->\n",
    "Weights             |  Image\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q2_4_weights.png)  |  ![](Q2_4_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5\n",
    "![](Q2_5.png) <br />\n",
    "We can see that there are some blocks that correspond to zero(0) and \"O\"; \"V\" and \"U\"; one(1) and \"I\"; five(5) and \"S\". \n",
    "Which means that the network is not able to distinguish these similar characters, which makes sense as well. They all have similar shapes and structure. So the weights corresponding them will have almost similar values and so will the gradient track the similar minima. Hence, we get these confused outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba93aa4dd47896315d502eafc29b2c6e",
     "grade": false,
     "grade_id": "cell-38104f92a19a284a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q3 (3 points)\n",
    "Show that the functions represented by a multi-layer fully-connected neural networks without a non-linear activation function are linear functions of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30381b92827e58e05fae85190eb15704",
     "grade": true,
     "grade_id": "cell-48454d1281c0524e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Let input be \"X\". And the function that maps from one layer (i) to the next (j) be $$F_{i,j} = W_{i,j}*X + b_{i,j}$$ For the first layer X will be the input, and for the subsequent layers X will be the output of the subsequent layers.\n",
    "$$F_{i,j} = W_{i,j}*F_{i-1,j-1} + b_{i,j}$$\n",
    "Here we can say that each next layer is a linear function of its previous layer.\n",
    "$$F_{i,j} = W_{i,j}*F(X) + b_{i,j}$$ \n",
    "If we keep tracking back to the first layer, $F_{i-1,j-1}$ will be a function of X.\n",
    "Therefore, every layer is a linear function of X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 (3 points, write-up)\n",
    "The method outlined above is pretty simplistic, and makes several assumptions. What are two big assumptions that the sample method makes. In your writeup, include two example images where you expect the character detection to fail (either miss valid letters, or respond to non-letters).\n",
    "\n",
    "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assumption: This algo assumes that the characters arent overwritten and have sufficient spaces between them. This will fail for cursive handwriting or cases where the characters are very close to each other. <br/>\n",
    "\n",
    "Second assumption: We also assume that each single character is fully connected and has no dis-joints, or is not made up of mixture of multiple small connections. <br/>\n",
    "\n",
    "This algorithm can classify bullet points as zeroes or Os, i.e., a non-letter. Or classify 5 as \"S\", 1 as \"I\" as miss valid letters. \n",
    "\n",
    "![](5.png) <br />\n",
    "![](bullet.png) <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3.2\n",
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "import skimage.measure\n",
    "import skimage.color\n",
    "import skimage.restoration\n",
    "import skimage.io\n",
    "import skimage.filters\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.restoration import denoise_tv_chambolle, estimate_sigma, denoise_bilateral\n",
    "from skimage.morphology import square\n",
    "\n",
    "# takes a color image\n",
    "# returns a list of bounding boxes and black_and_white image\n",
    "def findLetters(image):\n",
    "    image = rgb2gray(image)\n",
    "    image = img_as_float(image)\n",
    "    est_noise = estimate_sigma(image, average_sigmas=True)\n",
    "    # print(\"estimated noise = \", est_noise)\n",
    "    denoised_img = denoise_tv_chambolle(image, weight=0.1)\n",
    "    # denoised_img = denoise_bilateral(image, sigma_color=0.05, sigma_spatial=15)\n",
    "    \n",
    "    denoised_img = skimage.morphology.erosion(denoised_img, square(3))\n",
    "    denoised_img = skimage.morphology.erosion(denoised_img, square(3))\n",
    "    denoised_img = skimage.morphology.dilation(denoised_img)\n",
    "\n",
    "\n",
    "\n",
    "    bboxes = []\n",
    "    bw = None\n",
    "    # insert processing in here\n",
    "    # one idea estimate noise -> denoise -> greyscale -> threshold -> morphology -> label -> skip small boxes \n",
    "    # this can be 10 to 15 lines of code using skimage functions\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # apply threshold\n",
    "    thresh = threshold_otsu(denoised_img)\n",
    "    bw = denoised_img > thresh\n",
    "\n",
    "    # remove artifacts connected to image border\n",
    "    # cleared = clear_border(bw)\n",
    "\n",
    "    # label image regions\n",
    "    label_image = label(1-bw)\n",
    "    # to make the background transparent, pass the value of `bg_label`,\n",
    "    # and leave `bg_color` as `None` and `kind` as `overlay`\n",
    "    # image_label_overlay = label2rgb(label_image, image=denoised_img, bg_label=0)\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    # ax.imshow(image_label_overlay, cmap=\"gray\")\n",
    "\n",
    "    for region in regionprops(label_image):\n",
    "        # take regions with large enough areas\n",
    "        if region.area >= 50:\n",
    "            # draw rectangle around segmented coins\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            temp = [minr, minc, maxr, maxc]\n",
    "            # print(temp)\n",
    "            bboxes.append(temp)\n",
    "    # # raise NotImplementedError()\n",
    "    return bboxes, bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3.3\n",
    "import skimage\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "\n",
    "from ipynb.fs.defs.q1 import *\n",
    "\n",
    "# do not include any more libraries here!\n",
    "# no opencv, no sklearn, etc!\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "for img in os.listdir('images'):\n",
    "    im1 = skimage.img_as_float(skimage.io.imread(os.path.join('images',img)))\n",
    "    bboxes, bw = findLetters(im1)\n",
    "\n",
    "    plt.imshow(bw, cmap=\"gray\")\n",
    "    for bbox in bboxes:\n",
    "        minr, minc, maxr, maxc = bbox\n",
    "        rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.show()\n",
    "    # find the rows using..RANSAC, counting, clustering, etc.\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # raise NotImplementedError()\n",
    "    # crop the bounding boxes\n",
    "    # note.. before you flatten, transpose the image (that's how the dataset is!)\n",
    "    # consider doing a square crop, and even using np.pad() to get your images looking more like the dataset\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](Q3_3ABCD.png)  |  ![](Q3_3DeepLearning.png)\n",
    "![](Q3_3TODO.png)  |  ![](Q3_3Refrigator.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.4\n",
    "![](Q3_41.png)  |  ![](Q3_42.png)\n",
    "![](Q3_43.png)  |  ![](Q3_44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q3.4\n",
    "# load the weights\n",
    "# run the crops through your neural network and print them out\n",
    "import skimage\n",
    "import skimage.io\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "letters = np.array([_ for _ in string.ascii_uppercase[:26]] + [str(_) for _ in range(10)])\n",
    "params = pickle.load(open('q2_weights.pickle','rb'))\n",
    "# YOUR CODE HERE\n",
    "for img in os.listdir('images'):\n",
    "    im1 = skimage.img_as_float(skimage.io.imread(os.path.join('images',img)))\n",
    "    bboxes, bw = findLetters(im1)\n",
    "\n",
    "    plt.imshow(bw, cmap=\"gray\")\n",
    "    for bbox in bboxes:\n",
    "        minr, minc, maxr, maxc = bbox\n",
    "        rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "    sorted_temp = sorted(bboxes, key = lambda x:x[2])\n",
    "    lines = 1\n",
    "    max_chars_in_line = 0\n",
    "    chars_in_line = 0\n",
    "    # for i in range(1,len(sorted_temp)):\n",
    "    #     if abs(sorted_temp[i-1][2] - sorted_temp[i][2]) > 100:\n",
    "    #         # Append in the next row of sorted y\n",
    "    #         lines += 1\n",
    "    #         max_chars_in_line = max(chars_in_line, max_chars_in_line)\n",
    "    #         chars_in_line = 0\n",
    "    #     # sorted_y[x][y] = sorted_temp[i]\n",
    "    #     chars_in_line += 1\n",
    "\n",
    "    sorted_lines = []\n",
    "\n",
    "    # print(\"lines = \", lines)\n",
    "    # print(\"max cchars in line = \", max_chars_in_line)\n",
    "    # sorted_lines = np.ones((lines, max_chars_in_line, 4))*np.nan\n",
    "    curr_line = 0\n",
    "    curr_char_idx = 0\n",
    "    chars_in_one_line = []\n",
    "    for j in range(1,len(sorted_temp)):\n",
    "        if abs(sorted_temp[j-1][2] - sorted_temp[j][2]) > 100:\n",
    "            sorted_lines.append(chars_in_one_line)\n",
    "            chars_in_one_line = []\n",
    "            curr_line += 1\n",
    "            curr_char_idx = 0\n",
    "        # print(\"currline = \", curr_line)\n",
    "        # print(\"len = \", len(sorted_temp))\n",
    "        chars_in_one_line.append(sorted_temp[j])\n",
    "        curr_char_idx += 1\n",
    "\n",
    "    sorted_lines.append(chars_in_one_line)\n",
    "\n",
    "    for k in range(len(sorted_lines)):\n",
    "        sorted_lines[k] = sorted(sorted_lines[k], key = lambda x:x[3])\n",
    "\n",
    "    for i in sorted_lines:\n",
    "        for j in i:\n",
    "            patch = bw[j[0]:j[2],j[1]:j[3]]\n",
    "            # plt.imshow(patch, cmap='gray')\n",
    "            patch_ht = patch.shape[0]\n",
    "            patch_wdt = patch.shape[1]\n",
    "            pad_with = patch_ht-patch_wdt\n",
    "            if pad_with > 0:\n",
    "                patch = np.pad(patch, ((0,0),(pad_with//2, pad_with//2)), mode='constant', constant_values =1)\n",
    "            elif pad_with < 0:\n",
    "                patch = np.pad(patch, ((-pad_with//2, -pad_with//2),(0,0)), mode='constant', constant_values =1)\n",
    "\n",
    "            patch = np.pad(patch, ((25,25),(25,25)), mode='constant', constant_values =1)\n",
    "            patch = skimage.morphology.erosion(patch)\n",
    "            patch = skimage.transform.resize(patch, (32,32))\n",
    "            # plt.imshow(patch, cmap='gray')\n",
    "            patch = patch.transpose()\n",
    "\n",
    "            # plt.show()\n",
    "            patch = patch.reshape(1,1024)\n",
    "            post_act = forward(patch,params,'layer1',sigmoid)\n",
    "            pred_output = forward(post_act,params,'output',softmax)\n",
    "            pred_idx = np.argmax(pred_output[0])\n",
    "            detected_char = letters[pred_idx]\n",
    "            print(detected_char, end='')\n",
    "            # break\n",
    "        # break\n",
    "        print('')\n",
    "    # break\n",
    "        \n",
    "    \n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceef2df72991ea5b48aeb0ac12155489",
     "grade": false,
     "grade_id": "cell-23409e49f2f8eb65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4 (4 points) \n",
    "Given the sigmoid activation function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ , derive the gradient of the sigmoid function and show that it can be written solely as a function of $\\sigma(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f8509d97dc25f73bf21900d2e332113",
     "grade": true,
     "grade_id": "cell-c0ee4e1945d7414a",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$$\\frac{\\partial \\sigma(x)}{\\partial x} = \\frac{-(-e^{-x})}{(1+e^{-x})^2}\n",
    "                                        = \\sigma(x)*(1-\\sigma(x)) $$\n",
    "Therefore, the derivative of $\\sigma(x)$ can be expressed as a function of $\\sigma(x)$ itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1.1\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q4_11_accuracy.png)  |  ![](Q4_11_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from ipynb.fs.defs.q1 import *\n",
    "import scipy.io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "training_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "train_x, train_y = training_data['train_data'], training_data['train_labels']\n",
    "train_x, train_y = torch.from_numpy(train_x).float(),torch.from_numpy(train_y).float()\n",
    "\n",
    "# load_train_data = DataLoader(TensorDataset(train_x, train_y))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(1024, 64),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(64, 36),\n",
    "#             nn.Softmax(dim=1),\n",
    "#             # nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 64)\n",
    "        self.output = nn.Linear(64, 36)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = Network()\n",
    "\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    avg_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = nn.functional.cross_entropy(pred, y)\n",
    "        _, acc = compute_loss_and_acc(y.detach().numpy(), pred.detach().numpy())\n",
    "        avg_acc += acc/len(train_dataloader)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(\"Loss = \", loss)\n",
    "    print(\"Acc = \", avg_acc)\n",
    "    return avg_acc, loss.detach().numpy()\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "acc_arr = []\n",
    "loss_arr = []\n",
    "itr_arr = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    acc, l = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc_arr.append(acc)\n",
    "    loss_arr.append(l)\n",
    "    itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(itr_arr,acc_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(itr_arr,loss_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1.2\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q4_12_accuracy.png)  |  ![](Q4_12_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "# num_epochs = int(num_epochs)\n",
    "num_epochs = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    # 100, 32, 4, 4\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        print(\"image shape = \",images.size())\n",
    "        print(\"labels shape = \",labels.size())\n",
    "        # print(\"images size = \", images)\n",
    "        # print(\"labesl size = \", labels)\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            train_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            train_correct += (predicted == labels).sum()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "        train_loss_arr.append(train_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        print(\"Loss = \", train_loss)\n",
    "        print(\"Acc = \", train_accuracy)\n",
    "    # return train_acc_arr.detach().numpy(), train_loss_arr.detach().numpy()\n",
    "\n",
    "\n",
    "# def test(test_loader, model, loss_fn):           \n",
    "# # Calculate Accuracy         \n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     # Iterate through test dataset\n",
    "#     for images, labels in test_loader:\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             images = Variable(images.cuda())\n",
    "#         else:\n",
    "#             images = Variable(images)\n",
    "        \n",
    "#         # Forward pass only to get logits/output\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Loss\n",
    "#         test_loss = loss_fn(outputs, labels)\n",
    "\n",
    "#         # Get predictions from the maximum value\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "#         # Total number of labels\n",
    "#         test_total += labels.size(0)\n",
    "        \n",
    "#         # Total correct predictions\n",
    "#         if torch.cuda.is_available():\n",
    "#             test_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "#         else:\n",
    "#             test_correct += (predicted == labels).sum()\n",
    "    \n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "            \n",
    "            \n",
    "            # Print Loss\n",
    "            # print('Iteration: {}'.format(iter))\n",
    "            # print('Loss: {}'.format(loss.item()))\n",
    "            # print('Accuracy: {}'.format(accuracy.item()))\n",
    "\n",
    "train_acc_arr = []\n",
    "train_loss_arr = []\n",
    "itr_arr = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    # acc, l = train(train_loader, model, loss_fn, optimizer)\n",
    "    # train_acc_arr.append(acc)\n",
    "    # train_loss_arr.append(l)\n",
    "    # itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_acc_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_loss_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1.3\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q4_13_accuracy.png)  |  ![](Q4_13_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4.1.3\n",
    "# YOUR CODE HERE\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from ipynb.fs.defs.q1 import *\n",
    "import scipy.io\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "training_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "train_x, train_y = training_data['train_data'], training_data['train_labels']\n",
    "train_x, train_y = torch.from_numpy(train_x).float(),torch.from_numpy(train_y).float()\n",
    "\n",
    "# load_train_data = DataLoader(TensorDataset(train_x, train_y))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32*5*5, 36) \n",
    "    # 32, 5, 5\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1,32,32)\n",
    "        out = torch.unsqueeze(out,1)\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        # print(\"maxpool shape = \", out.size())\n",
    "        # out = out.view(out.size(0), -1)\n",
    "        out = torch.flatten(out,1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_accuracy = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, acc = compute_loss_and_acc(labels.detach().numpy(), outputs.detach().numpy())\n",
    "        train_accuracy += acc/len(train_loader)\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        # _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # # Total number of labels\n",
    "        # train_total += labels.size(0)\n",
    "        \n",
    "        # # Total correct predictions\n",
    "        # if torch.cuda.is_available():\n",
    "        #     train_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        # else:\n",
    "        #     train_correct += (predicted == labels).sum()\n",
    "\n",
    "        # train_accuracy = 100 * train_correct / train_total\n",
    "        # train_acc_arr.append(train_accuracy)\n",
    "        # train_loss_arr.append(train_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        print(\"Loss = \", train_loss)\n",
    "        print(\"Acc = \", train_accuracy)\n",
    "    return train_accuracy, train_loss.detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "train_acc_arr = []\n",
    "train_loss_arr = []\n",
    "itr_arr = []\n",
    "num_epochs = 30\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc, l = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_acc_arr.append(acc)\n",
    "    train_loss_arr.append(l)\n",
    "    itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_acc_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_loss_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1.4\n",
    "![](Q4_1_41.png)  |  ![](Q4_1_42.png)\n",
    "![](Q4_1_43.png)  |  ![](Q4_1_44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "train_dataset = dsets.EMNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True,                            \n",
    "                            split=\"balanced\")\n",
    "\n",
    "# test_dataset = dsets.EMNIST(root='./data', \n",
    "#                            train=False, \n",
    "#                            transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "# num_epochs = int(num_epochs)\n",
    "num_epochs = 5\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 47) \n",
    "    # 100, 32, 4, 4\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        # print(\"maxpool shape = \", out.size())\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "model = CNNModel()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        # print(\"images size = \", images.size())\n",
    "        # print(\"labesl size = \", labels.size())\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            train_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            train_correct += (predicted == labels).sum()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "        train_loss_arr.append(train_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        # print(\"Loss = \", train_loss)\n",
    "        # print(\"Acc = \", train_accuracy)\n",
    "    # return train_acc_arr.detach().numpy(), train_loss_arr\n",
    "\n",
    "train_acc_arr = []\n",
    "train_loss_arr = []\n",
    "itr_arr = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    # acc, l = train(train_loader, model, loss_fn, optimizer)\n",
    "    # train_acc_arr.append(acc)\n",
    "    # train_loss_arr.append(l)\n",
    "    # itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_acc_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_loss_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights\n",
    "# run the crops through your neural network and print them out\n",
    "import skimage\n",
    "import skimage.io\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from ipynb.fs.defs.q3 import *\n",
    "\n",
    "# letters = np.array([_ for _ in string.ascii_uppercase[:26]] + [str(_) for _ in range(10)])\n",
    "letters = train_dataset.classes\n",
    "params = pickle.load(open('q2_weights.pickle','rb'))\n",
    "# YOUR CODE HERE\n",
    "for img in os.listdir('images'):\n",
    "    im1 = skimage.img_as_float(skimage.io.imread(os.path.join('images',img)))\n",
    "    bboxes, bw = findLetters(im1)\n",
    "\n",
    "    plt.imshow(bw, cmap=\"gray\")\n",
    "    for bbox in bboxes:\n",
    "        minr, minc, maxr, maxc = bbox\n",
    "        rect = matplotlib.patches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "    sorted_temp = sorted(bboxes, key = lambda x:x[2])\n",
    "    lines = 1\n",
    "    max_chars_in_line = 0\n",
    "    chars_in_line = 0\n",
    "    # for i in range(1,len(sorted_temp)):\n",
    "    #     if abs(sorted_temp[i-1][2] - sorted_temp[i][2]) > 100:\n",
    "    #         # Append in the next row of sorted y\n",
    "    #         lines += 1\n",
    "    #         max_chars_in_line = max(chars_in_line, max_chars_in_line)\n",
    "    #         chars_in_line = 0\n",
    "    #     # sorted_y[x][y] = sorted_temp[i]\n",
    "    #     chars_in_line += 1\n",
    "\n",
    "    sorted_lines = []\n",
    "\n",
    "    # print(\"lines = \", lines)\n",
    "    # print(\"max cchars in line = \", max_chars_in_line)\n",
    "    # sorted_lines = np.ones((lines, max_chars_in_line, 4))*np.nan\n",
    "    curr_line = 0\n",
    "    curr_char_idx = 0\n",
    "    chars_in_one_line = []\n",
    "    for j in range(1,len(sorted_temp)):\n",
    "        if abs(sorted_temp[j-1][2] - sorted_temp[j][2]) > 100:\n",
    "            sorted_lines.append(chars_in_one_line)\n",
    "            chars_in_one_line = []\n",
    "            curr_line += 1\n",
    "            curr_char_idx = 0\n",
    "        # print(\"currline = \", curr_line)\n",
    "        # print(\"len = \", len(sorted_temp))\n",
    "        chars_in_one_line.append(sorted_temp[j])\n",
    "        curr_char_idx += 1\n",
    "\n",
    "    sorted_lines.append(chars_in_one_line)\n",
    "\n",
    "    for k in range(len(sorted_lines)):\n",
    "        sorted_lines[k] = sorted(sorted_lines[k], key = lambda x:x[3])\n",
    "\n",
    "    for i in sorted_lines:\n",
    "        for j in i:\n",
    "            patch = bw[j[0]:j[2],j[1]:j[3]]\n",
    "            # plt.imshow(patch, cmap='gray')\n",
    "            patch_ht = patch.shape[0]\n",
    "            patch_wdt = patch.shape[1]\n",
    "            pad_with = patch_ht-patch_wdt\n",
    "            if pad_with > 0:\n",
    "                patch = np.pad(patch, ((0,0),(pad_with//2, pad_with//2)), mode='constant', constant_values =1)\n",
    "            elif pad_with < 0:\n",
    "                patch = np.pad(patch, ((-pad_with//2, -pad_with//2),(0,0)), mode='constant', constant_values =1)\n",
    "\n",
    "            patch = np.pad(patch, ((25,25),(25,25)), mode='constant', constant_values =1)\n",
    "            patch = skimage.morphology.erosion(patch)\n",
    "            patch = skimage.transform.resize(patch, (32,32))\n",
    "            # plt.imshow(patch, cmap='gray')\n",
    "            patch = patch.transpose()\n",
    "\n",
    "            # plt.show()\n",
    "            patch = patch.reshape(1,1,32,32)\n",
    "            patch = patch[:,:,2:-2,2:-2]\n",
    "            patch = 1-patch\n",
    "            # post_act = forward(patch,params,'layer1',sigmoid)\n",
    "            # pred_output = forward(post_act,params,'output',softmax)\n",
    "            patch = torch.from_numpy(patch).float()\n",
    "            output = model(patch)\n",
    "            output = output.detach().numpy()\n",
    "            pred_idx = np.argmax(output[0])\n",
    "            detected_char = letters[pred_idx]\n",
    "            print(detected_char, end='')\n",
    "            # break\n",
    "        # break\n",
    "        print('')\n",
    "    # break\n",
    "        \n",
    "    \n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.2\n",
    "# For squeezenet:\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q4_2sqnest_acc.png)  |  ![](Q4_2sqnest_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)\n",
    "\n",
    "final_conv = nn.Conv2d(512, 17, kernel_size=1)\n",
    "model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.5), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
    "    )\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Construct an Optimizer object for updating the last layer only.\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# input_tensor = preprocess(input_image)\n",
    "# input_batch = input_tensor.unsqueeze(0) \n",
    "batch_size = 8\n",
    "train_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/train/', transform=preprocess), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/val/', transform=preprocess), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/test/', transform=preprocess), batch_size=340, shuffle=False)\n",
    "# print(\"len of data set = \", valid_dataloader.size())\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        # print(\"images size = \", images.size())\n",
    "        # print(\"labesl size = \", labels.size())\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            train_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            train_correct += (predicted == labels).sum()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "        train_loss_arr.append(train_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        print(\"Loss = \", train_loss)\n",
    "        print(\"Acc = \", train_accuracy)\n",
    "    # return train_acc_arr.detach().numpy(), train_loss_arr.detach().numpy()\n",
    "\n",
    "def valid(valid_loader, model, loss_fn):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        # print(\"images size = \", images.size())\n",
    "        # print(\"labesl size = \", labels.size())\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        valid_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        # valid_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        valid_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            valid_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            valid_correct += (predicted == labels).sum()\n",
    "\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "        valid_acc_arr.append(valid_accuracy)\n",
    "        valid_loss_arr.append(valid_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        print(\"Loss = \", valid_loss)\n",
    "        print(\"Acc = \", valid_accuracy)\n",
    "    # return valid_acc_arr.detach().numpy(), valid_loss_arr.detach().numpy()\n",
    "\n",
    "\n",
    "def test(test_loader, model, loss_fn):           \n",
    "# Calculate Accuracy         \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    # Iterate through test dataset\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Loss\n",
    "        test_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            test_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            test_correct += (predicted == labels).sum()\n",
    "\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        test_acc_arr.append(test_accuracy)\n",
    "        test_loss_arr.append(test_loss.detach().numpy())\n",
    "    \n",
    "            \n",
    "            \n",
    "            # Print Loss\n",
    "            # print('Iteration: {}'.format(iter))\n",
    "            # print('Loss: {}'.format(loss.item()))\n",
    "            # print('Accuracy: {}'.format(accuracy.item()))\n",
    "\n",
    "train_acc_arr = []\n",
    "valid_acc_arr = []\n",
    "test_acc_arr = []\n",
    "train_loss_arr = []\n",
    "test_loss_arr =[]\n",
    "valid_loss_arr = []\n",
    "itr_arr = []\n",
    "\n",
    "num_epochs = 5\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    valid(valid_dataloader, model, loss_fn)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    # acc, l = train(train_loader, model, loss_fn, optimizer)\n",
    "    # train_acc_arr.append(acc)\n",
    "    # train_loss_arr.append(l)\n",
    "    # itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_acc_arr, label = \"Train data\")\n",
    "plt.plot(np.arange(len(valid_acc_arr)),valid_acc_arr, label = \"Valid data\")\n",
    "plt.plot(np.arange(len(test_acc_arr)),test_acc_arr, label = \"Test data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_loss_arr, label = \"Train data\")\n",
    "plt.plot(np.arange(len(valid_acc_arr)),valid_loss_arr, label = \"Valid data\")\n",
    "plt.plot(np.arange(len(test_acc_arr)),test_loss_arr, label = \"Test data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "# raise NotImplementedError()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.2\n",
    "# For my network:\n",
    "3 conv layers, followed 2 fc layers\n",
    "Accuracy             |  Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q4_2mynet_acc.png)  |  ![](Q4_2mynet_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)\n",
    "\n",
    "# final_conv = nn.Conv2d(512, 17, kernel_size=1)\n",
    "# model.classifier = nn.Sequential(\n",
    "#         nn.Dropout(p=0.5), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Construct an Optimizer object for updating the last layer only.\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# input_tensor = preprocess(input_image)\n",
    "# input_batch = input_tensor.unsqueeze(0) \n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/train/', transform=preprocess), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/val/', transform=preprocess), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(dsets.ImageFolder('./data/oxford-flowers17/test/', transform=preprocess), batch_size=340, shuffle=False)\n",
    "# print(\"len of data set = \", valid_dataloader.size())\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "     \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn3 = nn.Conv2d(128, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Fully connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(153664, 47) \n",
    "        self.fc2 = nn.Linear(47, 17)\n",
    "    # 100, 32, 4, 4\n",
    "    def forward(self, x):\n",
    "        # Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        # Convolution 2 \n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "\n",
    "        out = self.cnn3(out)\n",
    "        out = self.relu3(out)\n",
    "        # print(\"maxpool shape = \", out.size())\n",
    "        # Resize\n",
    "        # Original size: (100, 32, 7, 7)\n",
    "        # out.size(0): 100\n",
    "        # New out size: (100, 32*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        # print(\"first conv shape = \", out.size())\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "model = CNNModel()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        # print(\"images size = \", images.size())\n",
    "        # print(\"labesl size = \", labels.size())\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        train_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            train_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            train_correct += (predicted == labels).sum()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_acc_arr.append(train_accuracy)\n",
    "        train_loss_arr.append(train_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        # print(\"Loss = \", train_loss)\n",
    "        # print(\"Acc = \", train_accuracy)\n",
    "    # return train_acc_arr.detach().numpy(), train_loss_arr.detach().numpy()\n",
    "\n",
    "def valid(valid_loader, model, loss_fn):\n",
    "    # iter = 0\n",
    "    # for epoch in range(num_epochs):\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    for i, (images, labels) in enumerate(valid_loader):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "        # print(\"images size = \", images.size())\n",
    "        # print(\"labesl size = \", labels.size())\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        # print(\"outputs  = \", outputs)\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        valid_loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        # valid_loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        # optimizer.step()\n",
    "        \n",
    "            # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        valid_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            valid_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            valid_correct += (predicted == labels).sum()\n",
    "\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "        valid_acc_arr.append(valid_accuracy)\n",
    "        valid_loss_arr.append(valid_loss.detach().numpy())\n",
    "        # itr_arr.append(passes)\n",
    "        # passes += 1\n",
    "        # print(\"Loss = \", valid_loss)\n",
    "        # print(\"Acc = \", valid_accuracy)\n",
    "    # return valid_acc_arr.detach().numpy(), valid_loss_arr.detach().numpy()\n",
    "\n",
    "\n",
    "def test(test_loader, model, loss_fn):           \n",
    "# Calculate Accuracy         \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    # Iterate through test dataset\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Loss\n",
    "        test_loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Total number of labels\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        # Total correct predictions\n",
    "        if torch.cuda.is_available():\n",
    "            test_correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "        else:\n",
    "            test_correct += (predicted == labels).sum()\n",
    "\n",
    "        test_accuracy = 100 * test_correct / test_total\n",
    "        test_acc_arr.append(test_accuracy)\n",
    "        test_loss_arr.append(test_loss.detach().numpy())\n",
    "    print(\"test accuracy = \", test_accuracy)\n",
    "            \n",
    "            \n",
    "            # Print Loss\n",
    "            # print('Iteration: {}'.format(iter))\n",
    "            # print('Loss: {}'.format(loss.item()))\n",
    "            # print('Accuracy: {}'.format(accuracy.item()))\n",
    "\n",
    "train_acc_arr = []\n",
    "valid_acc_arr = []\n",
    "test_acc_arr = []\n",
    "train_loss_arr = []\n",
    "test_loss_arr =[]\n",
    "valid_loss_arr = []\n",
    "itr_arr = []\n",
    "\n",
    "num_epochs = 5\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    valid(valid_dataloader, model, loss_fn)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "    # acc, l = train(train_loader, model, loss_fn, optimizer)\n",
    "    # train_acc_arr.append(acc)\n",
    "    # train_loss_arr.append(l)\n",
    "    # itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_acc_arr, label = \"Train data\")\n",
    "plt.plot(np.arange(len(valid_acc_arr)),valid_acc_arr, label = \"Valid data\")\n",
    "plt.plot(np.arange(len(test_acc_arr)),test_acc_arr, label = \"Test data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_acc_arr)),train_loss_arr, label = \"Train data\")\n",
    "plt.plot(np.arange(len(valid_acc_arr)),valid_loss_arr, label = \"Valid data\")\n",
    "plt.plot(np.arange(len(test_acc_arr)),test_loss_arr, label = \"Test data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "# raise NotImplementedError()\n",
    "# raise NotImplementedError()\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The performance of squeeznet is much better than my own network. (However I couldnt train my network using high number of hidden layer size due to not good enough compute.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab63520ddff815e9419408b34d681af4",
     "grade": false,
     "grade_id": "cell-b4ba2548034e7814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5 (12 points WriteUp)\n",
    "\n",
    "Given $y = W x + b$ (or $y_j = \\sum_{i=1}^d  x_{i} W_{ji} + b_j$), and the gradient of some loss $J$ with respect $y$, show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$. Be sure to do the derivatives with scalars and re-form the matrix form afterwards. Here are some helpful notations.\n",
    "$$ \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{k \\times 1} \\quad W \\in \\mathbb{R}^{k \\times d} \\quad x \\in \\mathbb{R}^{d \\times 1} \\quad b \\in \\mathbb{R}^{k \\times 1}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7489c88f7705b67573e66e63b4d066e6",
     "grade": true,
     "grade_id": "cell-d532eedba2ec9905",
     "locked": false,
     "points": 12,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "By chain rule: <br/>\n",
    "$$\\frac{\\partial J}{\\partial W} = \\sum_{i}^k\\frac{\\partial y_i}{\\partial W}*\\frac{\\partial J}{\\partial y_i}$$\n",
    "$$ Now, \\frac{\\partial y_i}{\\partial W_{rc}} = \\frac{\\partial \\sum_{j=1}^dW_{ij}x_j+b}{\\partial W_{rc}} $$\n",
    "$$ \\implies \\frac{\\partial y_i}{\\partial W_{rc}} = x_c for (i == r)$$ \n",
    "Since for every row in x we are getting the values in the columns first, we can express it as the transpose matrix.\n",
    "Therefore, $\\frac{\\partial y_i}{\\partial W_{rc}} = x^T$\n",
    "And $$\\frac{\\partial J}{\\partial W} = \\delta x^T which \\in \\mathbb{R}^{k \\times d}$$\n",
    "<br/>\n",
    "\n",
    "Now for: $\\frac{\\partial J}{\\partial x}$\n",
    "$$\\frac{\\partial J}{\\partial x} = \\sum_{i}^k\\frac{\\partial y_i}{\\partial x}*\\frac{\\partial J}{\\partial y_i}$$\n",
    "$$ Now, \\frac{\\partial y_i}{\\partial x_c} = \\frac{\\partial \\sum_{j=1}^dW_{ij}x_j+b}{\\partial x_c} $$\n",
    "$$ \\implies \\frac{\\partial y_i}{\\partial x_c} = W_{ic} $$ \n",
    "Therefore, $$\\frac{\\partial J}{\\partial x} = \\sum_{i}^k\\sum_{c}^dW_{ic}*\\frac{\\partial J}{\\partial y_i}$$\n",
    "Now similarly, if we check the matrix dimensonality we can figure that this is equivalent to multiplying by $W^T$.\n",
    "$$ \\implies \\frac{\\partial J}{\\partial x} = W^T*\\delta  \\in \\mathbb{R}^{d \\times 1}$$ \n",
    "<br/>\n",
    "\n",
    "Finally for $\\frac{\\partial J}{\\partial b}$\n",
    "$$\\frac{\\partial J}{\\partial b} = \\sum_{i}^k\\frac{\\partial y_i}{\\partial b}*\\frac{\\partial J}{\\partial y_i}$$\n",
    "Since it is just a scalar quantity getting added up, the derivative will be = 1. \n",
    "$$ \\implies \\frac{\\partial y_i}{\\partial b_j} = 1  for (i == j)$$ \n",
    "Therefore, $$\\frac{\\partial J}{\\partial b} = \\delta \\in \\mathbb{R}^{k \\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05edab7c7dad1246cddc3d17936b7b93",
     "grade": false,
     "grade_id": "cell-efa5d9ebad394fed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q6 (15 points)\n",
    "\n",
    "We will find the derivatives for Conv layers now. Since most Deep Learning frameworks such as Pytorch, Tensorflow use cross-correlation in their respective \"convolution\" functions ([Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/convolution)), we will continue this abuse of notation. So the operation performed with the Conv Layer weights will be cross-correlation.\n",
    "    \n",
    "The input, $x$ is of shape $M\\times N$ with C channels. This will be *convolved* (actually cross-correlation) with $D$ number of $K\\times K$ filters, each with a bias term. The stride is 1 and there will be no padding. We know the gradient of some loss $J$ with respect to the output $y$, which will have $D$ channels. Show how to get $\\frac{\\partial J}{\\partial W}$, $\\frac{\\partial J}{\\partial x}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "\n",
    "The dimensions and notation are as follows:\n",
    "$$\n",
    "    \\frac{\\partial J}{\\partial y} = \\delta \\in \\mathbb{R}^{D\\times M_o \\times N_o}\n",
    "    \\quad\n",
    "    M_o = M-K+1\n",
    "    \\quad\n",
    "    N_o = N-K+1\n",
    "$$\n",
    "$$\n",
    "    x \\in \\mathbb{R}^{C\\times M \\times N}\n",
    "    \\quad\n",
    "    W \\in \\mathbb{R}^{D\\times C \\times K \\times K}\n",
    "    \\quad\n",
    "    b \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "$x_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the input\n",
    "\n",
    "$y_{c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column and the $c^{th}$ channel of the output\n",
    "\n",
    "$W_{d, c, i, j}:$ The element at the $i^{th}$ row, the $j^{th}$ column, the $c^{th}$ channel of the kernel of the $d^{th}$ filter\n",
    "\n",
    "*For this question, you may compute the derivatives with scalars only. You don't need to re-form the matrix*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3624c404829a26b2682c1f41740dc2af",
     "grade": true,
     "grade_id": "cell-b02303c52179085f",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "\\begin{aligned}\n",
    "y_{d, i, j}= \\sum_{i^\\prime=0}^{K-1} \\sum_{j^\\prime=0}^{K-1} \\sum_{d=1}^d w_{d, c, g, h} x_{c, i+i^\\prime, j+j^\\prime}+b \\\\\n",
    "\\left(\\frac{\\partial J}{\\partial W}\\right){d, c, i, j}=\\sum{g=1}^{M_0} \\sum_{h=1}^{N_0}\\left(\\frac{\\partial J}{\\partial y_{d, g, h}}\\right)\\left(\\frac{\\partial y_{d, g, h}}{\\partial W_{d, c, i, j}}\\right) \\\\\n",
    "=\\sum_{g=1}^{M_0} \\sum_{h=1}^{N_0} \\delta_{d, g, h} \\frac{\\partial y_{d, g, h}}{\\partial W_{d, c, i, j}} \n",
    "=\\sum_{g=1}^{M_0} \\sum_{h=1}^{N_0} \\delta_{d, g, h} x_{c, g+i, h+j}\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\left(\\frac{\\partial J}{\\partial x}\\right){c, i, j} &=\\sum{g=0}^{M_0} \\sum_{h=0}^{N_0} \\sum_{d=1}^D\\left(\\frac{\\partial J}{\\partial y_{d, g, h}}\\right)\\left(\\frac{\\partial y_{d, g, h}}{\\partial x_{c, i, j}}\\right) \\\\\n",
    "&=\\sum_{g=0}^{M_0} \\sum_{h=0}^{N_0} \\sum_{d=1}^D \\delta_{d, g, h} W_{d, i-g, j-h}\n",
    "\\end{aligned}\n",
    "\n",
    "Wherever i-g and j-h aare negative, take $W_{d, i-g, j-h}$ to be 0.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\left(\\frac{\\partial J}{\\partial b}\\right){d} &=\\sum{g=1}^{M_0} \\sum_{h=1}^{N_0}\\left(\\frac{\\partial J}{\\partial y_{d, g, h}}\\right)\\left(\\frac{\\partial y_{d, g, h}}{\\partial b_d}\\right) \\\\\n",
    "&=\\sum_{g=1}^{M_0} \\sum_{h=1}^{N_0} \\delta_{d, g, h}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeeba4956195062fd9e58077f0fa9f8a",
     "grade": false,
     "grade_id": "cell-775c17b4eb8e7758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q7 (4 points)\n",
    "\n",
    "When the neural network applies the elementwise activation function (such as sigmoid), the gradient of the activation function scales the back-propagation update. This is directly from the chain rule, $\\frac{d}{d x} f(g(x)) = f'(g(x)) g'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b380b2b0ebb74f689d00c30fab3fdf26",
     "grade": false,
     "grade_id": "cell-4c0ef8ab732dda1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.1 (1 point)\n",
    "Consider the sigmoid activation function for deep neural networks. Why might it lead to a \"vanishing gradient\" problem if it is used for many layers (consider plotting the $\\sigma'(x)$ in Q1.4)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160f0d9b648792cd8f4b786a13d15578",
     "grade": true,
     "grade_id": "cell-cecc47088410bfc3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The major \"active area\" of the sigmoid function is very close to zero. So if we use this function multiple times, while back-propogating, we are multiplying very small numbers many times which will result in a quantity that is too small, thus eventuaally leading to vanishing of the gradient. After a point, the numerical underflow will occur which will make the gradient negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99bef598365d14fdd07e298ac59e55c5",
     "grade": false,
     "grade_id": "cell-c01f226eb7134484",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.2 (1 point)\n",
    "Often it is replaced with $\\tanh(x) = \\frac{1-e^{-2x}}{1+e^{-2x}}$. What are the output ranges of both $\\tanh$ and sigmoid? Why might we prefer $\\tanh$ ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f866c6c71afe439ec39b1b4c5b8012",
     "grade": true,
     "grade_id": "cell-359e85eddc00f567",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Output range of tanh: [-1,1] <br/>\n",
    "Output range of sigmoid: [0,1] <br/>\n",
    "Convergence is usually faster if the average of each input variable over the training set is close to zero. And the outputs using tanh centre around 0 rather than sigmoid's 0.5, and this \"makes learning for the next layer a little bit easier. Theefore we might prefre tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "889dabb3ca92c8a774f432172f58867e",
     "grade": false,
     "grade_id": "cell-adfb2179efada3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.3 (1 point)\n",
    "Why does $\\tanh(x)$ have less of a vanishing gradient problem? (plotting the derivatives helps! for reference: $\\tanh'(x) = 1 - \\tanh(x)^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5832ee3b18a8a57b28840e7ab6cb7f9e",
     "grade": true,
     "grade_id": "cell-c9f65d287cf03f87",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The response of tanh derivative as the input value approaches zero is larger than that of sigmoid's derivative. Therefore, the gradient will be magnified to a greater extent in case of tanh than sigmoid and hence vanshing gradient will be less of a problem if we use tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55afcde21a8360b35531aea4b919dd84",
     "grade": false,
     "grade_id": "cell-a1326ba9e015ed2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q7.4 (1 point)\n",
    "$\\tanh$ is a scaled and shifted version of the sigmoid. Show how $\\tanh(x)$ can be written in terms of $\\sigma(x)$. (*Hint: consider how to make it have the same range*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "965e794e6e31a8d771c8653489b4d1d6",
     "grade": true,
     "grade_id": "cell-1608d4ea95217d89",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$\\tanh(x) = \\frac{1-e^{-x}}{1+e^{-x}}$ <br/>\n",
    "$\\sigma(x) = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "$tanh(x) = 2*\\sigma(x) - 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.1.1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from ipynb.fs.defs.q1 import *\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "# we don't need labels now!\n",
    "train_x = train_data['train_data']\n",
    "valid_x = valid_data['valid_data']\n",
    "\n",
    "max_iters = 100\n",
    "# pick a batch size, initial learning rate\n",
    "batch_size = 3\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# raise NotImplementedError()\n",
    "hidden_size = 32\n",
    "lr_rate = 20\n",
    "\n",
    "batches = get_random_batches(train_x,np.ones((train_x.shape[0],1)),batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = Counter()\n",
    "\n",
    "# initialize layers here\n",
    "# YOUR CODE HERE\n",
    "initialize_weights(1024,hidden_size,params,'layer1')\n",
    "initialize_weights(hidden_size,hidden_size,params,'layer2')\n",
    "initialize_weights(hidden_size,hidden_size,params,'layer3')\n",
    "initialize_weights(hidden_size,1024,params,'layer4')\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.1.1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_translation(im, dx, dy):\n",
    "    '''\n",
    "    Applies a random translation to the image, described by dx, and dy.\n",
    "    \n",
    "    [input]\n",
    "    * im -- image to be translation\n",
    "    * dx -- the number of pixels the image should be translated in the x direction\n",
    "    * dy -- the number of pixesl the image should be translated in the y direction\n",
    "    [output]\n",
    "    * im -- the translated image\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    sequence = np.array([dx, dy, 3])\n",
    "    im_translated = ndimage.shift(im, sequence, mode='nearest')\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    return im_translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.1.2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_random_rotation(im, angle):\n",
    "    '''\n",
    "    Applies a random rotation to the image of angle degrees.\n",
    "    \n",
    "    [input]\n",
    "    * im -- image to be rotation\n",
    "    * angle -- the number of degrees for the image to be rotated.\n",
    "    \n",
    "    [output]\n",
    "    * im -- rotated image.\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    from PIL import Image\n",
    "    from scipy import misc\n",
    "    \n",
    "    # rotated_im = misc.ascent()\n",
    "    rotated_im = ndimage.rotate(im, angle, reshape=False, mode='nearest')\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    return rotated_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.1.3 code\n",
    "![](Q6_1_3_org.png)  |  ![](Q6_1_3_modified.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "# YOUR VISUALIZATION CODE HERE\n",
    "import matplotlib.pyplot as plt\n",
    "dx1 = 10\n",
    "dy1 = -10\n",
    "dx2 = 5\n",
    "dy2 = -5\n",
    "dx3 = -1\n",
    "dy3 = -1\n",
    "dx4 = 2\n",
    "dy4 = 3\n",
    "dx5 = -1\n",
    "dy5 = 0\n",
    "angle1 = 10\n",
    "angle2 = -90\n",
    "angle3 = 45\n",
    "angle4 = 180\n",
    "angle5 = -60\n",
    "\n",
    "print(\"input shape = \", train_x[0].shape)\n",
    "\n",
    "image1 = train_x[0].reshape(32,32, -1)\n",
    "image2 = train_x[1].reshape(32,32, -1)\n",
    "image3 = train_x[2].reshape(32,32, -1)\n",
    "image4 = train_x[3].reshape(32,32, -1)\n",
    "image5 = train_x[4].reshape(32,32, -1)\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 5))\n",
    "ax1, ax2, ax3, ax4, ax5 = fig2.subplots(1, 5)\n",
    "\n",
    "ax1.imshow(image1)\n",
    "ax2.imshow(image2)\n",
    "ax3.imshow(image3)\n",
    "ax4.imshow(image4)\n",
    "ax5.imshow(image5)\n",
    "plt.show()\n",
    "\n",
    "image1mod = apply_random_translation(image1, dx1, dy1)\n",
    "image1mod = apply_random_rotation(image1mod, angle1)\n",
    "image2mod = apply_random_translation(image2, dx2, dy2)\n",
    "image2mod = apply_random_rotation(image2mod, angle2)\n",
    "image3mod = apply_random_translation(image3, dx3, dy3)\n",
    "image3mod = apply_random_rotation(image3mod, angle3)\n",
    "image4mod = apply_random_translation(image4, dx4, dy4)\n",
    "image4mod = apply_random_rotation(image4mod, angle4)\n",
    "image5mod = apply_random_translation(image5, dx5, dy5)\n",
    "image5mod = apply_random_rotation(image5mod, angle5)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1, ax2, ax3, ax4, ax5 = fig.subplots(1, 5)\n",
    "\n",
    "ax1.imshow(image1mod)\n",
    "ax1.title.set_text('(10,-10,10)')\n",
    "ax2.imshow(image2mod)\n",
    "ax2.title.set_text('(5,-5,-90)')\n",
    "ax3.imshow(image3mod)\n",
    "ax3.title.set_text('(-1,-1,45)')\n",
    "ax4.imshow(image4mod)\n",
    "ax4.title.set_text('(2,3,180)')\n",
    "ax5.imshow(image5mod)\n",
    "ax5.title.set_text('(-1,0,-60)')\n",
    "# fig.set_layout_engine('tight')\n",
    "# image1mod = np.expand_dims(image1mod, axis=0)\n",
    "# plt.imshow(image1mod)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.2 code\n",
    "The accuracy for augmented data is less than that compared to non-augmented data. This is because we are modifying the original data for input but asking the network to learn it using the original data as the ground truth. Therefore, it takes more time for the network to learn the augmentation as well! And hence if we run it for the same parameters as that of the non-augmented data the accuracy will be less.\n",
    "Augmented Accuracy             |  Augmented Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q6_2_aug_accuracy.png)  |  ![](Q6_2_aug_loss.png)\n",
    "\n",
    "Non-Augmented Accuracy             |  Non-Augmented Loss\n",
    ":-------------------------:|:-------------------------:\n",
    "![](Q6_2_nonaug_accuracy.png)  |  ![](Q6_2_nonaug_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "valid_data = scipy.io.loadmat('data/nist36_valid.mat')\n",
    "\n",
    "train_x, train_y = train_data['train_data'], train_data['train_labels']\n",
    "valid_x, valid_y = valid_data['valid_data'], valid_data['valid_labels']\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    x = np.random.randint(-6,6)\n",
    "    y = np.random.randint(-6,6)\n",
    "    ang = np.random.randint(-45,45)\n",
    "    image = train_x[i].reshape(32,32, -1) \n",
    "    image = apply_random_translation(image, x, y)\n",
    "    train_x[i] = apply_random_rotation(image, ang).ravel()\n",
    "# apply_random_translation(image1, dx1, dy1)\n",
    "# image1mod = apply_random_rotation(image1mod, angle1)\n",
    "\n",
    "max_iters = 50\n",
    "# pick a batch size, learning rate\n",
    "batch_size = 3\n",
    "learning_rate = 1e-3\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# raise NotImplementedError()\n",
    "hidden_size = 64\n",
    "\n",
    "batches = get_random_batches(train_x,train_y,batch_size)\n",
    "batch_num = len(batches)\n",
    "\n",
    "params = {}\n",
    "\n",
    "# initialize layers (named \"layer1\" and \"output\") here\n",
    "# YOUR CODE HERE\n",
    "initialize_weights(1024,hidden_size,params,'layer1')\n",
    "initialize_weights(hidden_size,36,params,'output')\n",
    "# raise NotImplementedError()\n",
    "train_acc_arr = []\n",
    "train_loss_arr = []\n",
    "valid_acc_arr = []\n",
    "valid_loss_arr = []\n",
    "itr_arr = []\n",
    "# with default settings, you should get loss < 150 and accuracy > 80%\n",
    "for itr in range(max_iters):\n",
    "    itr_arr.append(itr)\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    for xb,yb in batches:\n",
    "        \n",
    "        # print(\"xb shape = \", xb.shape)\n",
    "        # training loop can be exactly the same as q2!\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        post_act = forward(xb,params,'layer1',sigmoid)\n",
    "        pred_output = forward(post_act,params,'output',softmax)\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        # loss\n",
    "        # be sure to add loss and accuracy to epoch totals\n",
    "        # YOUR CODE HERE\n",
    "        loss, acc = compute_loss_and_acc(yb, pred_output)\n",
    "        total_loss += loss/len(batches)\n",
    "        total_acc += acc/len(batches)\n",
    "        \n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        # backward\n",
    "        # YOUR CODE HERE\n",
    "        last_layer_backprop = backwards(pred_output - yb, params, 'output', linear_deriv)\n",
    "        hidden_layer_backprop  = backwards(last_layer_backprop, params, 'layer1', sigmoid_deriv)\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "        # apply gradient\n",
    "        # YOUR CODE HERE\n",
    "        params['Woutput'] = params['Woutput'] - learning_rate*params['grad_Woutput']\n",
    "        params['boutput'] = params['boutput'] - learning_rate*params['grad_boutput']\n",
    "        params['Wlayer1'] = params['Wlayer1'] - learning_rate*params['grad_Wlayer1']\n",
    "        params['blayer1'] = params['blayer1'] - learning_rate*params['grad_blayer1']\n",
    "            \n",
    "        # raise NotImplementedError()\n",
    "    train_acc_arr.append(total_acc)\n",
    "    train_loss_arr.append(total_loss)\n",
    "    if itr % 2 == 0:\n",
    "        print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(itr,total_loss,total_acc))\n",
    "\n",
    "# run on validation set and report accuracy! should be above 70%\n",
    "    \n",
    "    post_act = forward(valid_x,params,'layer1',sigmoid)\n",
    "    pred_output = forward(post_act,params,'output',softmax)\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    # loss\n",
    "    # be sure to add loss and accuracy to epoch totals\n",
    "    # YOUR CODE HERE\n",
    "    loss, acc = compute_loss_and_acc(valid_y, pred_output)\n",
    "    valid_loss += loss/len(batches)\n",
    "    valid_acc += acc\n",
    "    valid_acc_arr.append(valid_acc)\n",
    "    valid_loss_arr.append(valid_loss)\n",
    "    # raise NotImplementedError()\n",
    "    print('Validation accuracy: ',valid_acc)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(itr_arr,train_acc_arr, label = \"Train data\")\n",
    "plt.plot(itr_arr,valid_acc_arr, label = \"Valid data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(itr_arr,train_loss_arr, label = \"Train data\")\n",
    "plt.plot(itr_arr,valid_loss_arr, label = \"Valid data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from ipynb.fs.defs.q1 import *\n",
    "import scipy.io\n",
    "import torch.nn.functional as F\n",
    "\n",
    "training_data = scipy.io.loadmat('data/nist36_train.mat')\n",
    "train_x, train_y = training_data['train_data'], training_data['train_labels']\n",
    "train_x, train_y = torch.from_numpy(train_x).float(),torch.from_numpy(train_y).float()\n",
    "\n",
    "# load_train_data = DataLoader(TensorDataset(train_x, train_y))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(TensorDataset(train_x, train_y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(1024, 64),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Linear(64, 36),\n",
    "#             nn.Softmax(dim=1),\n",
    "#             # nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "# model = NeuralNetwork().to(device)\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 64)\n",
    "        self.output = nn.Linear(64, 36)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = F.log_softmax(x,dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = Network()\n",
    "\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    avg_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = nn.functional.cross_entropy(pred, y)\n",
    "        _, acc = compute_loss_and_acc(y.detach().numpy(), pred.detach().numpy())\n",
    "        avg_acc += acc/len(train_dataloader)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    print(\"Loss = \", loss)\n",
    "    print(\"Acc = \", avg_acc)\n",
    "    return avg_acc, loss.detach().numpy()\n",
    "\n",
    "\n",
    "epochs = 50\n",
    "acc_arr = []\n",
    "loss_arr = []\n",
    "itr_arr = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    acc, l = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    acc_arr.append(acc)\n",
    "    loss_arr.append(l)\n",
    "    itr_arr.append(t)\n",
    "print(\"Done!\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(itr_arr,acc_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(itr_arr,loss_arr, label = \"Train data\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
