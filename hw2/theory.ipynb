{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7788918",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ba35ab89cdc3e39c5704e8e4e387627",
     "grade": false,
     "grade_id": "cell-2f0bf8de83a87eae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Theory Questions\n",
    "\n",
    "This section should include the visualizations and answers to specifically highlighted questions from P1 to P4. This section will be manually Graded "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8640212",
   "metadata": {},
   "source": [
    "### Q1.1.2 Manually grade the image\n",
    "\n",
    "![](Q1(1)image1.png) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63fdff",
   "metadata": {},
   "source": [
    "### Q1.2.1 Manually grade 5 images for Harris Corner\n",
    "\n",
    "![](Q1(1_2)image1.png) <br />\n",
    "![](Q1(1_2)image2.png) <br />\n",
    "![](Q1(1_2)image3.png) <br />\n",
    "![](Q1(1_2)image4.png) <br />\n",
    "![](Q1(1_2)image5.png) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9f0eb7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "199f0b423bb79cdeb8d761ce8f9df98d",
     "grade": false,
     "grade_id": "cell-d2e7501d5ec1729e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q1.1.1 (5 Points WriteUp)\n",
    "What visual properties do each of the filter functions (See Figure below) pick up? You should group the filters into categories by its purpose/functionality. Also, why do we need multiple scales of filter responses? **Answer in the writeup. Answer in your write-up.**\n",
    "\n",
    "<img align=\"center\" src=\"figures/filters_image.png\" width=\"500\">\n",
    "<figcaption align=\"center\"><b>Figure1. The provided multi-scale filter bank</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ab82c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "deca7d19a5637c50906e02a8d6c4877f",
     "grade": true,
     "grade_id": "cell-f20eebb8abbd872b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "First horizontal layer:- Gaussian Blur <br />\n",
    "Second horizontal layer:- Gaussian laplace <br />\n",
    "Third horizontal layer:- Gaussian in x-direction <br />\n",
    "Fourth horizontal layer:- Gaussian in y-direction <br />\n",
    "\n",
    "Gaussian blur smoothens the image (removes high frequency content) and eventually we get a relatively high response at a greater intensity region. \n",
    "Gaussian laplace picks up points which exhibit a particular high light intensity in a better way as compared to the other filters (gaussian blur) as far as direction is concerned. \n",
    "Gaussian in x and Gaussian in y picks up edges in x and y direction respictively.\n",
    "\n",
    "We can see that with the use of different types of filters we can extract different kinds of features. For ex, using Gaussian in x, y we can detect edges and corners. But using just one of them wont give us corners neither will we have any information about the other direction that we didnt use. Also, a combination of gaussian blur and gaussian laplace will give us some sort of rotational invariance. Therefore, to extract features in a better way, it makes sense to use a combination of different types of filters. \n",
    "\n",
    "Now, even in one filter we use different scales! This is to capture features of different sizes. We can say that using different scales makes our feature extraction scale invarient. This can be very well understood by the example of sunflower that we dicsussed in the class. We get the maximum response for one particular jscale for a particular feature. And while extracting the feature, that is the response we will use!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda6631",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b6856c73e0ec7654f58f2bfa2e8d7f",
     "grade": false,
     "grade_id": "cell-f8136fffb67fc66f",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q1.3.1 (5 Points WriteUp)\n",
    "\n",
    "Visualize three wordmaps of images from any one of the category. **Include these in your write-up, along with the original RGB images. Include some comments on these visualizations: do the “word” boundaries make sense to you?**. We have provided helper function to save and visualize the resulting wordmap in the util.py file. They should look similar to the ones in Figure 2.\n",
    "\n",
    "<img align=\"center\" src=\"./figures/textons.jpg\" width=\"800\">\n",
    "<figcaption align = \"center\"><b>Figure 2. Visual words over images. You will use the spatially un-ordered distribution of visual words in a region (a bag of visual words) as a feature for scene classification, with some coarse information provided by spatial pyramid matching [2]</b></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5bcf3",
   "metadata": {},
   "source": [
    "We can see from the following wordmaps when compared with the original image, that similar kind of features (for example car, sign boards, bridges, sky, etc) are expressed with similar colors (i.e., wordmaps). Yes, the \"word\" boundaries make sense when we compare the images. All the different features have different colors (words) and all the same ones have similar ones. And these boundaries can be seen on the wordmaps.\n",
    "\n",
    "![](Q131.png) <br />\n",
    "![](sun_aacqsbumiuidokeh.jpg) <br />\n",
    "<br />\n",
    "![](Q132.png) <br />\n",
    "![](sun_acpvugnkzrliaqir.jpg) <br />\n",
    "\n",
    "![](Q133.png) <br />\n",
    "![](sun_advstbacygihnsur.jpg) <br />\n",
    "\n",
    "![](Q134.png) <br />\n",
    "![](sun_afdslvedyijtrjof.jpg) <br />\n",
    "\n",
    "![](Q135.png) <br />\n",
    "![](sun_akihkricraebyttv1.jpg) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6b431",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "456c33594131e9a50908bfcc7b703f4e",
     "grade": false,
     "grade_id": "cell-2cf410e4507cf87f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q2.1\n",
    "**For 5 Images, include their visual word maps and histograms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b6192",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6aa3b80c369eccfad7ee7aa4509d0770",
     "grade": true,
     "grade_id": "cell-f8873a304123ee24",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "![](Q2(1)(image1).png) <br />\n",
    "![](Q211.png) <br />\n",
    "![](Q2(1)(image2).png) <br />\n",
    "![](Q212.png) <br />\n",
    "![](Q2(1)(image3).png) <br />\n",
    "![](Q213.png) <br />\n",
    "![](Q2(1)(image4).png) <br />\n",
    "![](Q214.png) <br />\n",
    "![](Q2(1)(image5).png) <br />\n",
    "![](Q215.png) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b53b24",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2391f7ef74f5d6be7b06374f7d41de",
     "grade": false,
     "grade_id": "cell-f11c4f53168fabbf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.1 \n",
    "Submit the visualization of Confusion Matrix and the Accuracy value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e901a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "189f94a62bb1a83b1fb8a933028e5306",
     "grade": true,
     "grade_id": "cell-a67d219e82ac3ea5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Accuracy = 0.48125\n",
    "![](Q3matrix.png) <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58420900",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5219c1bfeff05a50e6c52fb438b4f120",
     "grade": false,
     "grade_id": "cell-c77fa30dd0533616",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Q3.1.2 (5 points WriteUp):\n",
    "\n",
    "As there are some classes/samples that are more difficult to classify than the rest using the bags-of-words approach, they are more easily classified incorrectly into other categories. **List some of these classes/samples and discuss why they are more difficult.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0af44b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aeaa24fd50837bd0204227238782e828",
     "grade": true,
     "grade_id": "cell-fe8e3fd47e21e13c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "For classes that have a lot of similar features its difficult to classify them seperately. For example, an aquarium that has water color as light blue and an image of a park where very less greenery but a lot of open sky (with a color similar to the one we dicsussed before), the classification will be easily messed up. Another example would be a kitchen and a laundromat. In both the cases there is a lot of structre to the enviornment. And therefore it will be difficult to classify them differently. In our particular case, we can see from the confusion matrix that highway and windmill (3,7) got classified as similar relatively high when they should'nt have been. And it makes sense after looking at the images under those labels as there are some windmill images which also have roads in them. Similarly with, windmill and park (7,1). There are images in windmill that have also got trees and green lawns and therefore the algorithms aligns them with park.\n",
    "\n",
    "In general, it becomes difficult to classify such similar classes/samples because there is similar structure of the individual features that the images consist of and hence while comparing the test image with the bag of words to create the histogram, similar features (that are not actually same and belong to some different class) get mapped incorrectly. And hence leading to incorrect classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4e800",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2da804b4ebd6680ded96429b206041e1",
     "grade": false,
     "grade_id": "cell-a0d4cf029c9816a6",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.3 [Extra Credit](10 points) Manually Graded:\n",
    "\n",
    "Now that you have seen how well your recognition system can perform on a set of real images, you can experiment with different ways of improving this baseline system. \n",
    "\n",
    "Include the changes, modification you made and the impact it had on accuracy.\n",
    "\n",
    "Tune the system you build to reach around 65\\% accuracy on the provided test set (``data/test_data.npz``). **In your writeup, document what you did to achieve such performance: (1) what you did, (2) what you expected would happen, and (3) what actually happened.** Also, include a file called ``custom.py/ipynb`` for running your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d7f31",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b04b30519c4d03f7064609fb83fb0d7",
     "grade": true,
     "grade_id": "cell-b7979e73bac0c915",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3da0b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c65c949456220674e7a42ef4653c40ce",
     "grade": false,
     "grade_id": "cell-0ab5de6e6222b473",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q3.1.4 [Extra Credit] (5 points write up):\n",
    "**GIST feature descriptor:** As introduced during the lecture, GIST feature descriptor is a feature extractor based on Gabor Filters. When we apply it to images, we have to implement the 2D Gabor Filters as described below\n",
    "\n",
    "<img align=\"center\" src=\"figures/gist.png\" width=\"800\">\n",
    "\n",
    "<font color=\"blue\">**In your writeup: How does GIST descriptor affect the performance? Better or worse? Explain your reasoning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01349c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91cebc6bce44b037e5405cebd599c2bb",
     "grade": true,
     "grade_id": "cell-8949e75ea938cd42",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d934de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b0bd3e412021431ff9479e9e99f5084",
     "grade": false,
     "grade_id": "cell-5a254c9a47e7f561",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q4.2.1 (2 points write up)\n",
    "**Report the confusion matrix and accuracy for your results in your write-up. Can you comment in your writeup on whether the results are better or worse than classical BoW - why do you think that is?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece79aea",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12da5faa2c3fc2315a366648c12b87d4",
     "grade": true,
     "grade_id": "cell-c383f7a8536d254d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The results in case of using CNNs to extract and identify/classify features are significantly better than than that in case of Bag of Words algorithm. This is not surprising as in CNNs we have got much more paramaters that can be tuned and learnt to represent the features in images. Thus providing a much robust basis for feature recognition. Also, the availabe data features of trained images in the algorithm we're using (VGG16) is HUGE (4096) as compared to just 200 in case of Bag of Words that we are making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a3c6a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a04a7ff6ad689e2079f8c0ed82baa3c",
     "grade": false,
     "grade_id": "cell-81f8a97ac34fe774",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Q4.3.2 [Extra Credit] (2 points write up)\n",
    "**Report the confusion matrix and accuracy for your ViT results in your write-up. Can you comment in your writeup on whether the results are better or worse than VGG - why do you think that is? A short answer is okay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9fd65a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "420edc8366b3e0d107b30a2d677a6df7",
     "grade": true,
     "grade_id": "cell-dc8ff4b969a16c8a",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060890e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aff4e09d015af0ae2a101a7f03fcc8a9",
     "grade": false,
     "grade_id": "cell-39235682903e017c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### References\n",
    "\n",
    "[1]  James Hays and Alexei A Efros. Scene completion using millions of photographs.ACM Transactions onGraphics (SIGGRAPH 2007), 26(3), 2007.\n",
    "\n",
    "[2]  S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for recogniz-ing natural scene categories.  InComputer Vision and Pattern Recognition (CVPR), 2006 IEEE Conferenceon, volume 2, pages 2169–2178, 2006.\n",
    "\n",
    "[3]  Jian xiong Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recogni-tion from abbey to zoo.2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pages 3485–3492, 2010.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745cc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
